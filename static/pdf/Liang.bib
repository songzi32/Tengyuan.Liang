
@inproceedings{belloniEscapingLocalMinima2015a,
  title = {Escaping the Local Minima via Simulated Annealing: {{Optimization}} of Approximately Convex Functions},
  booktitle = {Proceedings of {{The Conference}} on {{Learning Theory}} ({{COLT}})},
  author = {Belloni, Alexandre and Liang, Tengyuan and Narayanan, Hariharan and Rakhlin, Alexander},
  year = {2015},
  month = jul,
  volume = {40},
  pages = {240--265},
  publisher = {{PMLR}},
  abstract = {We consider the problem of optimizing an approximately convex function over a bounded convex set in \textsuperscript{n} using only function evaluations. The problem is reduced to sampling from an log-concave distribution using the Hit-and-Run method, which is shown to have the same \textsuperscript{*} complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an {$\epsilon$}-minimizer after \textsuperscript{*}(n{$^7$}.5{$\epsilon{}^-$}2) noisy function evaluations by inducing a ({$\epsilon$}/n)-approximately log concave distribution. We also consider in detail the case when the ``amount of non-convexity'' decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.},
  pdf = {http://proceedings.mlr.press/v40/Belloni15.pdf},
  place = {Paris, France}
}

@article{caiComputationalStatisticalBoundaries2017a,
  title = {Computational and Statistical Boundaries for Submatrix Localization in a Large Noisy Matrix},
  author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
  year = {2017},
  month = aug,
  volume = {45},
  pages = {1403--1430},
  issn = {0090-5364},
  doi = {10.1214/16-AOS1488},
  file = {/Users/tengyuan/Zotero/storage/VADU8E9W/Cai et al. - 2017 - Computational and statistical boundaries for subma.pdf},
  journal = {The Annals of Statistics},
  language = {English},
  number = {4}
}

@article{caiDetectionStructuralReconstruction2017a,
  title = {On {{Detection}} and {{Structural Reconstruction}} of {{Small}}-{{World Random Networks}}},
  author = {Cai, Tony and Liang, Tengyuan and Rakhlin, Alexander},
  year = {2017},
  month = jul,
  volume = {4},
  pages = {165--176},
  issn = {2327-4697},
  doi = {10.1109/TNSE.2017.2703102},
  file = {/Users/tengyuan/Zotero/storage/HXR2BV8Q/Cai et al. - 2017 - On Detection and Structural Reconstruction of Smal.pdf},
  journal = {IEEE Transactions on Network Science and Engineering},
  number = {3}
}

@article{caiGeometricInferenceGeneral2016a,
  title = {Geometric Inference for General High-Dimensional Linear Inverse Problems},
  author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
  year = {2016},
  month = aug,
  volume = {44},
  pages = {1536--1563},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1426},
  file = {/Users/tengyuan/Zotero/storage/MLG5SLJA/Cai et al. - 2016 - Geometric inference for general high-dimensional l.pdf},
  journal = {The Annals of Statistics},
  language = {English},
  number = {4}
}

@article{caiLawLogDeterminant2015a,
  title = {Law of Log Determinant of Sample Covariance Matrix and Optimal Estimation of Differential Entropy for High-Dimensional {{Gaussian}} Distributions},
  author = {Cai, T. Tony and Liang, Tengyuan and Zhou, Harrison H.},
  year = {2015},
  month = may,
  volume = {137},
  pages = {161--172},
  issn = {0047259X},
  doi = {10.1016/j.jmva.2015.02.003},
  file = {/Users/tengyuan/Zotero/storage/YUSGNZIB/Cai et al. - 2015 - Law of log determinant of sample covariance matrix.pdf},
  journal = {Journal of Multivariate Analysis},
  language = {English}
}

@article{caiWeightedMessagePassing2020,
  title = {Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information},
  author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
  year = {2020},
  month = jan,
  volume = {21},
  pages = {1--34},
  journal = {Journal of Machine Learning Research},
  number = {11}
}

@article{douTrainingNeuralNetworks2020,
  title = {Training {{Neural Networks}} as {{Learning Data}}-Adaptive {{Kernels}}: {{Provable Representation}} and {{Approximation Benefits}}},
  shorttitle = {Training {{Neural Networks}} as {{Learning Data}}-Adaptive {{Kernels}}},
  author = {Dou, Xialiang and Liang, Tengyuan},
  year = {2020},
  month = mar,
  pages = {1--35},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2020.1745812},
  file = {/Users/tengyuan/Zotero/storage/3VRNHWPE/Dou and Liang - 2020 - Training Neural Networks as Learning Data-adaptive.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en}
}

@article{farrellDeepNeuralNetworks2019,
  title = {Deep {{Neural Networks}} for {{Estimation}} and {{Inference}}},
  author = {Farrell, Max H. and Liang, Tengyuan and Misra, Sanjog},
  year = {2019},
  month = sep,
  abstract = {We study deep neural networks and their use in semiparametric inference. We establish novel rates of convergence for deep feedforward neural nets. Our new rates are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our estimation rates and semiparametric inference results handle the current standard architecture: fully connected feedforward neural networks (multi-layer perceptrons), with the now-common rectified linear unit activation function and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, such as treatment effects, expected welfare, and decomposition effects. Inference in many other semiparametric contexts can be readily obtained. We demonstrate the effectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct mail marketing.},
  archivePrefix = {arXiv},
  eprint = {1809.09953},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/I8YE647S/Farrell et al. - 2019 - Deep Neural Networks for Estimation and Inference.pdf;/Users/tengyuan/Zotero/storage/FKAP55VV/1809.html},
  journal = {arXiv:1809.09953 [cs, econ, math, stat]},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, econ, math, stat}
}

@inproceedings{kaleAdaptiveFeatureSelection2017a,
  title = {Adaptive Feature Selection: {{Computationally}} Efficient Online Sparse Linear Regression under {{RIP}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kale, Satyen and Karnin, Zohar and Liang, Tengyuan and P{\'a}l, D{\'a}vid},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {1780--1788},
  publisher = {{PMLR}},
  abstract = {Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.},
  pdf = {http://proceedings.mlr.press/v70/kale17a/kale17a.pdf},
  place = {International Convention Centre, Sydney, Australia}
}

@article{liangEstimatingCertainIntegral2019,
  title = {Estimating {{Certain Integral Probability Metric}} ({{IPM}}) Is as {{Hard}} as {{Estimating}} under the {{IPM}}},
  author = {Liang, Tengyuan},
  year = {2019},
  month = nov,
  abstract = {We study the minimax optimal rates for estimating a range of Integral Probability Metrics (IPMs) between two unknown probability measures, based on \$n\$ independent samples from them. Curiously, we show that estimating the IPM itself between probability measures, is not significantly easier than estimating the probability measures under the IPM. We prove that the minimax optimal rates for these two problems are multiplicatively equivalent, up to a \$\textbackslash{}log \textbackslash{}log (n)/\textbackslash{}log (n)\$ factor.},
  archivePrefix = {arXiv},
  eprint = {1911.00730},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/67AW2AWY/Liang - 2019 - Estimating Certain Integral Probability Metric (IP.pdf;/Users/tengyuan/Zotero/storage/E2LIWDLH/1911.html},
  journal = {arXiv:1911.00730 [math, stat]},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {math, stat}
}

@inproceedings{liangFisherraoMetricGeometry2019a,
  title = {Fisher-Rao Metric, Geometry, and Complexity of Neural Networks},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {888--896},
  publisher = {{PMLR}},
  abstract = {We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity \textemdash{} the Fisher-Rao norm \textemdash{} that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.},
  pdf = {http://proceedings.mlr.press/v89/liang19a/liang19a.pdf}
}

@article{liangHowWellGenerative2019,
  title = {On {{How Well Generative Adversarial Networks Learn Densities}}: {{Nonparametric}} and {{Parametric Results}}},
  shorttitle = {On {{How Well Generative Adversarial Networks Learn Densities}}},
  author = {Liang, Tengyuan},
  year = {2019},
  month = jun,
  abstract = {We study in this paper the rate of convergence for learning distributions with the adversarial framework and Generative Adversarial Networks (GANs), which subsumes Wasserstein, Sobolev and MMD GANs as special cases. We study a wide range of parametric and nonparametric target distributions, under a collection of objective evaluation metrics. On the nonparametric end, we investigate the minimax optimal rates and fundamental difficulty of the density estimation under the adversarial framework. On the parametric end, we establish a theory for general neural network classes (including deep leaky ReLU as a special case), that characterizes the interplay on the choice of generator and discriminator. We investigate how to obtain a good statistical guarantee for GANs through the lens of regularization. We discover and isolate a new notion of regularization, called the \textbackslash{}textit\{generator/discriminator pair regularization\}, that sheds light on the advantage of GANs compared to classical parametric and nonparametric approaches for density estimation. We develop novel oracle inequalities as the main tools for analyzing GANs, which is of independent theoretical interest.},
  archivePrefix = {arXiv},
  eprint = {1811.03179},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/RW9YLRGW/Liang - 2019 - On How Well Generative Adversarial Networks Learn .pdf;/Users/tengyuan/Zotero/storage/33VIZW6T/1811.html},
  journal = {arXiv:1811.03179 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@inproceedings{liangInteractionMattersNote2019a,
  title = {Interaction Matters: {{A}} Note on Non-Asymptotic Local Convergence of Generative Adversarial Networks},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Liang, Tengyuan and Stokes, James},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {907--915},
  publisher = {{PMLR}},
  abstract = {Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for four modified dynamics proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO), Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate. As a by-product, we present a new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017] with improved rates.},
  pdf = {http://proceedings.mlr.press/v89/liang19b/liang19b.pdf}
}

@article{liangJustInterpolateKernel2019,
  title = {Just {{Interpolate}}: {{Kernel}} "{{Ridgeless}}" {{Regression Can Generalize}}},
  shorttitle = {Just {{Interpolate}}},
  author = {Liang, Tengyuan and Rakhlin, Alexander},
  year = {2019},
  month = feb,
  abstract = {In the absence of explicit regularization, Kernel "Ridgeless" Regression with nonlinear kernels has the potential to fit the training data perfectly. It has been observed empirically, however, that such interpolated solutions can still generalize well on test data. We isolate a phenomenon of implicit regularization for minimum-norm interpolated solutions which is due to a combination of high dimensionality of the input data, curvature of the kernel function, and favorable geometric properties of the data such as an eigenvalue decay of the empirical covariance and kernel matrices. In addition to deriving a data-dependent upper bound on the out-of-sample error, we present experimental evidence suggesting that the phenomenon occurs in the MNIST dataset.},
  archivePrefix = {arXiv},
  eprint = {1808.00387},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/HE577UTB/Liang and Rakhlin - 2019 - Just Interpolate Kernel Ridgeless Regression Ca.pdf;/Users/tengyuan/Zotero/storage/ATXK4Q7C/1808.html},
  journal = {arXiv:1808.00387 [cs, math, stat], The Annals of Statistics, to appear},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@inproceedings{liangLearningSquareLoss2015a,
  title = {Learning with Square Loss: {{Localization}} through Offset Rademacher Complexity},
  booktitle = {Proceedings of {{The Conference}} on {{Learning Theory}} ({{COLT}})},
  author = {Liang, Tengyuan and Rakhlin, Alexander and Sridharan, Karthik},
  year = {2015},
  month = jul,
  volume = {40},
  pages = {1260--1285},
  publisher = {{PMLR}},
  abstract = {We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of 15 for the bounded case while also providing guarantees without the boundedness assumption.},
  pdf = {http://proceedings.mlr.press/v40/Liang15.pdf},
  place = {Paris, France}
}

@article{liangMultipleDescentMinimumNorm2020,
  title = {On the {{Multiple Descent}} of {{Minimum}}-{{Norm Interpolants}} and {{Restricted Lower Isometry}} of {{Kernels}}},
  author = {Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu},
  year = {2020},
  month = feb,
  abstract = {We study the risk of minimum-norm interpolants of data in Reproducing Kernel Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape for the various scalings of \$d = n\^\{\textbackslash{}alpha\}\$, \$\textbackslash{}alpha\textbackslash{}in(0,1)\$, for the input dimension \$d\$ and sample size \$n\$. Empirical evidence supports our finding that minimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in sample size; furthermore, locations of the peaks in our experiments match our theoretical predictions. Since gradient flow on appropriately initialized wide neural networks converges to a minimum-norm interpolant with respect to a certain kernel, our analysis also yields novel estimation and generalization guarantees for these over-parametrized models. At the heart of our analysis is a study of spectral properties of the random kernel matrix restricted to a filtration of eigen-spaces of the population covariance operator, and may be of independent interest.},
  archivePrefix = {arXiv},
  eprint = {1908.10292},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/PIGPNCWK/Liang et al. - 2020 - On the Multiple Descent of Minimum-Norm Interpolan.pdf;/Users/tengyuan/Zotero/storage/KVQU26RE/1908.html},
  journal = {arXiv:1908.10292 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{liangPreciseHighDimensionalAsymptotic2020,
  title = {A {{Precise High}}-{{Dimensional Asymptotic Theory}} for {{Boosting}} and {{Min}}-{{L1}}-{{Norm Interpolated Classifiers}}},
  author = {Liang, Tengyuan and Sur, Pragya},
  year = {2020},
  month = feb,
  abstract = {This paper establishes a precise high-dimensional asymptotic theory for Boosting on separable data, taking statistical and computational perspectives. We consider the setting where the number of features (weak learners) p scales with the sample size n, in an over-parametrized regime. On the statistical front, we provide an exact analysis of the generalization error of Boosting, when the algorithm interpolates the training data and maximizes an empirical L1 margin. The angle between the Boosting solution and the ground truth is characterized explicitly. On the computational front, we provide a sharp analysis of the stopping time when Boosting approximately maximizes the empirical L1 margin. Furthermore, we discover that, the larger the margin, the smaller the proportion of active features (with zero initialization). At the heart of our theory lies a detailed study of the maximum L1 margin, using tools from convex geometry. The maximum L1 margin can be precisely described by a new system of non-linear equations, which we study using a novel uniform deviation argument. Preliminary numerical results are presented to demonstrate the accuracy of our theory.},
  archivePrefix = {arXiv},
  eprint = {2002.01586},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/STHQ4N4K/Liang and Sur - 2020 - A Precise High-Dimensional Asymptotic Theory for B.pdf;/Users/tengyuan/Zotero/storage/4AF9PESV/2002.html},
  journal = {arXiv:2002.01586 [cs, math, stat]},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{liangStatisticalInferencePopulation2019,
  title = {Statistical Inference for the Population Landscape via Moment-Adjusted Stochastic Gradients},
  author = {Liang, Tengyuan and Su, Weijie J.},
  year = {2019},
  month = apr,
  volume = {81},
  pages = {431--456},
  issn = {13697412},
  doi = {10.1111/rssb.12313},
  file = {/Users/tengyuan/Zotero/storage/27WXHNAY/Liang and Su - 2019 - Statistical inference for the population landscape.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  language = {en},
  number = {2}
}

@inproceedings{tzenLocalOptimalityGeneralization2018a,
  title = {Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability},
  booktitle = {Proceedings of the {{Conference On Learning Theory}} ({{COLT}})},
  author = {Tzen, Belinda and Liang, Tengyuan and Raginsky, Maxim},
  year = {2018},
  month = jul,
  volume = {75},
  pages = {857--875},
  publisher = {{PMLR}},
  abstract = {We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \emph{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the {$\varepsilon$}-neighborhood of this particular optimum within a short \emph{recurrence time}; (2) it enters this {$\varepsilon$}-neighborhood by the recurrence time and stays there until a potentially exponentially long \emph{escape time}. We call this phenomenon \emph{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality.},
  pdf = {http://proceedings.mlr.press/v75/tzen18a/tzen18a.pdf}
}


