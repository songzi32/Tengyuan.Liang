%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for tengyuan at 2020-07-22 21:24:46 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{CAI2015161,
	Abstract = {Differential entropy and log determinant of the covariance matrix of a multivariate Gaussian distribution have many applications in coding, communications, signal processing and statistical inference. In this paper we consider in the high-dimensional setting optimal estimation of the differential entropy and the log-determinant of the covariance matrix. We first establish a central limit theorem for the log determinant of the sample covariance matrix in the high-dimensional setting where the dimension p(n) can grow with the sample size n. An estimator of the differential entropy and the log determinant is then considered. Optimal rate of convergence is obtained. It is shown that in the case p(n)/n→0 the estimator is asymptotically sharp minimax. The ultra-high-dimensional setting where p(n)>n is also discussed.},
	Author = {T. Tony Cai and Tengyuan Liang and Harrison H. Zhou},
	Date-Added = {2020-07-22 21:21:30 -0500},
	Date-Modified = {2020-07-22 21:21:30 -0500},
	Doi = {https://doi.org/10.1016/j.jmva.2015.02.003},
	Issn = {0047-259X},
	Journal = {Journal of Multivariate Analysis},
	Keywords = {Asymptotic optimality, Central limit theorem, Covariance matrix, Determinant, Differential entropy, Minimax lower bound, Sharp minimaxity},
	Pages = {161 - 172},
	Title = {Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional Gaussian distributions},
	Url = {http://www.sciencedirect.com/science/article/pii/S0047259X1500038X},
	Volume = {137},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0047259X1500038X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jmva.2015.02.003}}

@article{cai2016,
	Author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2020-07-22 21:20:52 -0500},
	Date-Modified = {2020-07-22 21:20:52 -0500},
	Doi = {10.1214/15-AOS1426},
	Fjournal = {Annals of Statistics},
	Journal = {Ann. Statist.},
	Month = {08},
	Number = {4},
	Pages = {1536--1563},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Geometric inference for general high-dimensional linear inverse problems},
	Url = {https://doi.org/10.1214/15-AOS1426},
	Volume = {44},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1214/15-AOS1426}}

@article{7924316,
	Abstract = {In this paper, we study detection and fast reconstruction of the celebrated Watts-Strogatz (WS) small-world random graph model [29] which aims to describe real-world complex networks that exhibit both high clustering and short average length properties. The WS model with neighborhood size k and rewiring probability probability β can be viewed as a continuous interpolation between a deterministic ring lattice graph and the Erdos-Renyi random graph. We study the computational and statistical aspects of detection and recovery of the deterministic ring lattice structure (strong ties) in the presence of random connections (weak ties). The phase diagram in terms of (k, β) is shown to consist of several regions according to the difficulty of the problem. We propose distinct methods for these regions.},
	Author = {T. {Cai} and T. {Liang} and A. {Rakhlin}},
	Date-Added = {2020-07-22 21:20:28 -0500},
	Date-Modified = {2020-07-22 21:20:28 -0500},
	Doi = {10.1109/TNSE.2017.2703102},
	Issn = {2327-4697},
	Journal = {IEEE Transactions on Network Science and Engineering},
	Keywords = {graph theory;interpolation;fast reconstruction;Watts-Strogatz small-world random graph model;real-world complex networks;WS model;Erdos-Renyi random graph;continuous interpolation;deterministic ring lattice graph;Lattices;Symmetric matrices;Structural rings;Computational modeling;Mathematical model;Complex networks;Testing;Small world networks;random graphs;spectral analysis;detection;reconstruction;computational boundary},
	Month = {July},
	Number = {3},
	Pages = {165-176},
	Title = {On Detection and Structural Reconstruction of Small-World Random Networks},
	Volume = {4},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNSE.2017.2703102}}

@article{cai2017,
	Author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2020-07-22 21:19:55 -0500},
	Date-Modified = {2020-07-22 21:19:55 -0500},
	Doi = {10.1214/16-AOS1488},
	Fjournal = {Annals of Statistics},
	Journal = {Ann. Statist.},
	Month = {08},
	Number = {4},
	Pages = {1403--1430},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Computational and statistical boundaries for submatrix localization in a large noisy matrix},
	Url = {https://doi.org/10.1214/16-AOS1488},
	Volume = {45},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1214/16-AOS1488}}

@article{doi:10.1080/01621459.2020.1745812,
	Abstract = { AbstractConsider the problem: given the data pair (x,y) drawn from a population with f*(x)=E[y|x=x], specify a neural network model and run gradient flow on the weights over time until reaching any stationarity. How does ft, the function computed by the neural network at time t, relate to f*, in terms of approximation and representation? What are the provable benefits of the adaptive representation by neural networks compared to the prespecified fixed basis representation in the classical nonparametric literature? We answer the above questions via a dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the training process of neural networks. First, we show that when reaching any local stationarity, gradient flow learns an adaptive RKHS representation and performs the global least-squares projection onto the adaptive RKHS, simultaneously. Second, we prove that as the RKHS is data-adaptive and task-specific, the residual for f* lies in a subspace that is potentially much smaller than the orthogonal complement of the RKHS. The result formalizes the representation and approximation benefits of neural networks. Finally, we show that the neural network function computed by gradient flow converges to the kernel ridgeless regression with an adaptive kernel, in the limit of vanishing regularization. The adaptive kernel viewpoint provides new angles of studying the approximation, representation, generalization, and optimization advantages of neural networks. Supplementary materials for this article are available online. },
	Author = {Xialiang Dou and Tengyuan Liang},
	Date-Added = {2020-07-22 21:18:24 -0500},
	Date-Modified = {2020-07-22 21:18:24 -0500},
	Doi = {10.1080/01621459.2020.1745812},
	Eprint = {https://doi.org/10.1080/01621459.2020.1745812},
	Journal = {Journal of the American Statistical Association},
	Number = {0},
	Pages = {1-14},
	Publisher = {Taylor & Francis},
	Title = {Training Neural Networks as Learning Data-Adaptive Kernels: Provable Representation and Approximation Benefits},
	Url = {https://doi.org/10.1080/01621459.2020.1745812},
	Volume = {0},
	Year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2020.1745812}}

@article{doi:10.1111/rssb.12313,
	Abstract = {Summary Modern statistical inference tasks often require iterative optimization methods to compute the solution. Convergence analysis from an optimization viewpoint informs us only how well the solution is approximated numerically but overlooks the sampling nature of the data. In contrast, recognizing the randomness in the data, statisticians are keen to provide uncertainty quantification, or confidence, for the solution obtained by using iterative optimization methods. The paper makes progress along this direction by introducing moment-adjusted stochastic gradient descent: a new stochastic optimization method for statistical inference. We establish non-asymptotic theory that characterizes the statistical distribution for certain iterative methods with optimization guarantees. On the statistical front, the theory allows for model misspecification, with very mild conditions on the data. For optimization, the theory is flexible for both convex and non-convex cases. Remarkably, the moment adjusting idea motivated from `error standardization' in statistics achieves a similar effect to acceleration in first-order optimization methods that are used to fit generalized linear models. We also demonstrate this acceleration effect in the non-convex setting through numerical experiments.},
	Author = {Liang, Tengyuan and Su, Weijie J.},
	Date-Added = {2020-07-22 21:17:08 -0500},
	Date-Modified = {2020-07-22 21:17:08 -0500},
	Doi = {10.1111/rssb.12313},
	Eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12313},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Keywords = {Acceleration, Diffusion process, Discretized Langevin algorithm, Model misspecification, Non-asymptotic inference, Population landscape, Stochastic gradient methods},
	Number = {2},
	Pages = {431-456},
	Title = {Statistical inference for the population landscape via moment-adjusted stochastic gradients},
	Url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12313},
	Volume = {81},
	Year = {2019},
	Bdsk-Url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12313},
	Bdsk-Url-2 = {https://doi.org/10.1111/rssb.12313}}

@article{JMLR:v21:18-573,
	Author = {T. Tony Cai and Tengyuan Liang and Alexander Rakhlin},
	Date-Added = {2020-07-22 21:16:15 -0500},
	Date-Modified = {2020-07-22 21:16:15 -0500},
	Journal = {Journal of Machine Learning Research},
	Number = {11},
	Pages = {1-34},
	Title = {Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information},
	Url = {http://jmlr.org/papers/v21/18-573.html},
	Volume = {21},
	Year = {2020},
	Bdsk-Url-1 = {http://jmlr.org/papers/v21/18-573.html}}

@article{liang2020,
	Author = {Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2020-07-22 21:12:30 -0500},
	Date-Modified = {2020-07-22 21:12:30 -0500},
	Doi = {10.1214/19-AOS1849},
	Fjournal = {Annals of Statistics},
	Journal = {Ann. Statist.},
	Month = {06},
	Number = {3},
	Pages = {1329--1347},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Just interpolate: Kernel ``Ridgeless'' regression can generalize},
	Url = {https://doi.org/10.1214/19-AOS1849},
	Volume = {48},
	Year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1214/19-AOS1849}}
