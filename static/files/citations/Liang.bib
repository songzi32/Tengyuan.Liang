
@inproceedings{belloni2015EscapingLocal,
  title = {Escaping the Local Minima via Simulated Annealing: {{Optimization}} of Approximately Convex Functions},
  booktitle = {Proceedings of the 28th Conference on Learning Theory},
  author = {Belloni, Alexandre and Liang, Tengyuan and Narayanan, Hariharan and Rakhlin, Alexander},
  editor = {Gr{\"u}nwald, Peter and Hazan, Elad and Kale, Satyen},
  year = {2015},
  month = jul,
  volume = {40},
  pages = {240--265},
  publisher = {{PMLR}},
  address = {{Paris, France}},
  abstract = {We consider the problem of optimizing an approximately convex function over a bounded convex set in \textsuperscript{n} using only function evaluations. The problem is reduced to sampling from an log-concave distribution using the Hit-and-Run method, which is shown to have the same \textsuperscript{*} complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an {$\epsilon$}-minimizer after \textsuperscript{*}(n{$^7$}.5{$\epsilon^-$}2) noisy function evaluations by inducing a ({$\epsilon$}/n)-approximately log concave distribution. We also consider in detail the case when the ``amount of non-convexity'' decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.},
  date-added = {2020-07-22 20:53:46 -0500},
  date-modified = {2020-07-22 20:53:46 -0500},
  file = {/Users/tengyuan/Zotero/storage/HQ8DB3SD/Belloni et al. - 2015 - Escaping the Local Minima via Simulated Annealing.pdf},
  pdf = {http://proceedings.mlr.press/v40/Belloni15.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{cai2015LawLoga,
  ids = {cai2015LawLog},
  title = {Law of Log Determinant of Sample Covariance Matrix and Optimal Estimation of Differential Entropy for High-Dimensional {{Gaussian}} Distributions},
  author = {Cai, T. Tony and Liang, Tengyuan and Zhou, Harrison H.},
  year = {2015},
  volume = {137},
  pages = {161--172},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2015.02.003},
  abstract = {Differential entropy and log determinant of the covariance matrix of a multivariate Gaussian distribution have many applications in coding, communications, signal processing and statistical inference. In this paper we consider in the high-dimensional setting optimal estimation of the differential entropy and the log-determinant of the covariance matrix. We first establish a central limit theorem for the log determinant of the sample covariance matrix in the high-dimensional setting where the dimension p(n) can grow with the sample size n. An estimator of the differential entropy and the log determinant is then considered. Optimal rate of convergence is obtained. It is shown that in the case p(n)/n\textrightarrow 0 the estimator is asymptotically sharp minimax. The ultra-high-dimensional setting where p(n)\textquestiondown n is also discussed.},
  bdsk-url-2 = {https://doi.org/10.1016/j.jmva.2015.02.003},
  date-added = {2020-07-22 21:21:30 -0500},
  date-modified = {2020-07-22 21:21:30 -0500},
  file = {/Users/tengyuan/Zotero/storage/CEQIRPAX/Cai et al. - 2015 - Law of log determinant of sample covariance matrix.pdf},
  journal = {Journal of Multivariate Analysis},
  keywords = {Asymptotic optimality,Central limit theorem,Covariance matrix,Determinant,Differential entropy,Minimax lower bound,Sharp minimaxity}
}

@article{cai2016GeometricInferencea,
  ids = {cai2016GeometricInference},
  title = {Geometric Inference for General High-Dimensional Linear Inverse Problems},
  author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
  year = {2016},
  month = aug,
  volume = {44},
  pages = {1536--1563},
  publisher = {{The Institute of Mathematical Statistics}},
  doi = {10.1214/15-AOS1426},
  date-added = {2020-07-22 21:20:52 -0500},
  date-modified = {2020-07-22 21:20:52 -0500},
  file = {/Users/tengyuan/Zotero/storage/37ZGDJYK/Cai et al. - 2016 - Geometric inference for general high-dimensional l.pdf},
  fjournal = {Annals of Statistics},
  journal = {The Annals of Statistics},
  number = {4}
}

@article{cai2017ComputationalStatistical,
  ids = {cai2017ComputationalStatistical},
  title = {Computational and Statistical Boundaries for Submatrix Localization in a Large Noisy Matrix},
  author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
  year = {2017},
  month = aug,
  volume = {45},
  pages = {1403--1430},
  publisher = {{The Institute of Mathematical Statistics}},
  doi = {10.1214/16-AOS1488},
  date-added = {2020-07-22 21:19:55 -0500},
  date-modified = {2020-07-22 21:19:55 -0500},
  file = {/Users/tengyuan/Zotero/storage/Z8UI2CYU/Cai et al. - 2017 - Computational and statistical boundaries for subma.pdf},
  fjournal = {Annals of Statistics},
  journal = {The Annals of Statistics},
  number = {4}
}

@article{cai2017DetectionStructural,
  ids = {cai2017DetectionStructural},
  title = {On Detection and Structural Reconstruction of Small-World Random Networks},
  author = {Cai, T. and Liang, T. and Rakhlin, A.},
  year = {2017},
  month = jul,
  volume = {4},
  pages = {165--176},
  issn = {2327-4697},
  doi = {10.1109/TNSE.2017.2703102},
  abstract = {In this paper, we study detection and fast reconstruction of the celebrated Watts-Strogatz (WS) small-world random graph model [29] which aims to describe real-world complex networks that exhibit both high clustering and short average length properties. The WS model with neighborhood size k and rewiring probability probability {$\beta$} can be viewed as a continuous interpolation between a deterministic ring lattice graph and the Erdos-Renyi random graph. We study the computational and statistical aspects of detection and recovery of the deterministic ring lattice structure (strong ties) in the presence of random connections (weak ties). The phase diagram in terms of (k, {$\beta$}) is shown to consist of several regions according to the difficulty of the problem. We propose distinct methods for these regions.},
  date-added = {2020-07-22 21:20:28 -0500},
  date-modified = {2020-07-22 21:20:28 -0500},
  file = {/Users/tengyuan/Zotero/storage/C2XDICXK/Cai et al. - 2017 - On Detection and Structural Reconstruction of Smal.pdf},
  journal = {IEEE Transactions on Network Science and Engineering},
  keywords = {Complex networks,computational boundary,Computational modeling,continuous interpolation,detection,deterministic ring lattice graph,Erdos-Renyi random graph,fast reconstruction,graph theory,interpolation,Lattices,Mathematical model,random graphs,real-world complex networks,reconstruction,Small world networks,spectral analysis,Structural rings,Symmetric matrices,Testing,Watts-Strogatz small-world random graph model,WS model},
  number = {3}
}

@article{cai2020WeightedMessage,
  ids = {cai2020WeightedMessage},
  title = {Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information},
  author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
  year = {2020},
  volume = {21},
  pages = {1--34},
  date-added = {2020-07-22 21:16:15 -0500},
  date-modified = {2020-07-22 21:16:15 -0500},
  file = {/Users/tengyuan/Zotero/storage/Y88CIPSX/Cai et al. - 2020 - Weighted Message Passing and Minimum Energy Flow f.pdf},
  journal = {Journal of Machine Learning Research},
  number = {11}
}

@article{dou2020TrainingNeural,
  ids = {dou2020TrainingNeural},
  title = {Training Neural Networks as Learning Data-Adaptive Kernels: {{Provable}} Representation and Approximation Benefits},
  author = {Dou, Xialiang and Liang, Tengyuan},
  year = {2020},
  volume = {0},
  pages = {1--14},
  publisher = {{Taylor \& Francis}},
  doi = {10.1080/01621459.2020.1745812},
  abstract = {AbstractConsider the problem: given the data pair (x,y) drawn from a population with f*(x)=E[y|x=x], specify a neural network model and run gradient flow on the weights over time until reaching any stationarity. How does ft, the function computed by the neural network at time t, relate to f*, in terms of approximation and representation? What are the provable benefits of the adaptive representation by neural networks compared to the prespecified fixed basis representation in the classical nonparametric literature? We answer the above questions via a dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the training process of neural networks. First, we show that when reaching any local stationarity, gradient flow learns an adaptive RKHS representation and performs the global least-squares projection onto the adaptive RKHS, simultaneously. Second, we prove that as the RKHS is data-adaptive and task-specific, the residual for f* lies in a subspace that is potentially much smaller than the orthogonal complement of the RKHS. The result formalizes the representation and approximation benefits of neural networks. Finally, we show that the neural network function computed by gradient flow converges to the kernel ridgeless regression with an adaptive kernel, in the limit of vanishing regularization. The adaptive kernel viewpoint provides new angles of studying the approximation, representation, generalization, and optimization advantages of neural networks. Supplementary materials for this article are available online.},
  date-added = {2020-07-22 21:18:24 -0500},
  date-modified = {2020-07-22 21:18:24 -0500},
  eprint = {https://doi.org/10.1080/01621459.2020.1745812},
  file = {/Users/tengyuan/Zotero/storage/CECJIH3G/Dou and Liang - 2020 - Training Neural Networks as Learning Data-adaptive.pdf},
  journal = {Journal of the American Statistical Association},
  number = {0}
}

@article{farrell2018DeepNeural,
  ids = {farrell2018DeepNeuralNetworksa},
  title = {Deep {{Neural Networks}} for {{Estimation}} and {{Inference}}},
  author = {Farrell, Max H. and Liang, Tengyuan and Misra, Sanjog},
  year = {2018},
  month = sep,
  abstract = {We study deep neural networks and their use in semiparametric inference. We establish novel rates of convergence for deep feedforward neural nets. Our new rates are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our estimation rates and semiparametric inference results handle the current standard architecture: fully connected feedforward neural networks (multi-layer perceptrons), with the now-common rectified linear unit activation function and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, such as treatment effects, expected welfare, and decomposition effects. Inference in many other semiparametric contexts can be readily obtained. We demonstrate the effectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct mail marketing.},
  archivePrefix = {arXiv},
  eprint = {1809.09953},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/LJMVDZAH/Farrell et al. - 2019 - Deep Neural Networks for Estimation and Inference.pdf},
  journal = {arXiv:1809.09953},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@inproceedings{kale2017AdaptiveFeature,
  title = {Adaptive Feature Selection: {{Computationally}} Efficient Online Sparse Linear Regression under {{RIP}}},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Kale, Satyen and Karnin, Zohar and Liang, Tengyuan and P{\'a}l, D{\'a}vid},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {1780--1788},
  publisher = {{PMLR}},
  address = {{International Convention Centre, Sydney, Australia}},
  abstract = {Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.},
  date-added = {2020-07-22 20:53:32 -0500},
  date-modified = {2020-07-22 20:53:32 -0500},
  file = {/Users/tengyuan/Zotero/storage/SQAJKQC6/Kale et al. - 2017 - Adaptive Feature Selection Computationally Effici.pdf},
  pdf = {http://proceedings.mlr.press/v70/kale17a/kale17a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{liang2015LearningSquare,
  title = {Learning with Square Loss: {{Localization}} through Offset Rademacher Complexity},
  booktitle = {Proceedings of the 28th Conference on Learning Theory},
  author = {Liang, Tengyuan and Rakhlin, Alexander and Sridharan, Karthik},
  editor = {Gr{\"u}nwald, Peter and Hazan, Elad and Kale, Satyen},
  year = {2015},
  month = jul,
  volume = {40},
  pages = {1260--1285},
  publisher = {{PMLR}},
  address = {{Paris, France}},
  abstract = {We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of 15 for the bounded case while also providing guarantees without the boundedness assumption.},
  date-added = {2020-07-22 20:53:40 -0500},
  date-modified = {2020-07-22 20:53:40 -0500},
  file = {/Users/tengyuan/Zotero/storage/NXEKPVND/Liang et al. - 2015 - Learning with Square Loss Localization through Of.pdf},
  pdf = {http://proceedings.mlr.press/v40/Liang15.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{liang2018HowWell,
  ids = {liang2018HowWellGenerative},
  title = {On {{How Well Generative Adversarial Networks Learn Densities}}: {{Nonparametric}} and {{Parametric Results}}},
  shorttitle = {On {{How Well Generative Adversarial Networks Learn Densities}}},
  author = {Liang, Tengyuan},
  year = {2018},
  month = nov,
  abstract = {We study in this paper the rate of convergence for learning distributions with the adversarial framework and Generative Adversarial Networks (GANs), which subsumes Wasserstein, Sobolev and MMD GANs as special cases. We study a wide range of parametric and nonparametric target distributions, under a collection of objective evaluation metrics. On the nonparametric end, we investigate the minimax optimal rates and fundamental difficulty of the density estimation under the adversarial framework. On the parametric end, we establish a theory for general neural network classes (including deep leaky ReLU as a special case), that characterizes the interplay on the choice of generator and discriminator. We investigate how to obtain a good statistical guarantee for GANs through the lens of regularization. We discover and isolate a new notion of regularization, called the \textbackslash textit\{generator/discriminator pair regularization\}, that sheds light on the advantage of GANs compared to classical parametric and nonparametric approaches for density estimation. We develop novel oracle inequalities as the main tools for analyzing GANs, which is of independent theoretical interest.},
  archivePrefix = {arXiv},
  eprint = {1811.03179},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/KP5RM7IG/Liang - 2019 - On How Well Generative Adversarial Networks Learn .pdf},
  journal = {arXiv:1811.03179},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{liang2019EstimatingCertain,
  ids = {liang2019EstimatingCertainIntegrala},
  title = {Estimating {{Certain Integral Probability Metric}} ({{IPM}}) Is as {{Hard}} as {{Estimating}} under the {{IPM}}},
  author = {Liang, Tengyuan},
  year = {2019},
  month = nov,
  abstract = {We study the minimax optimal rates for estimating a range of Integral Probability Metrics (IPMs) between two unknown probability measures, based on \$n\$ independent samples from them. Curiously, we show that estimating the IPM itself between probability measures, is not significantly easier than estimating the probability measures under the IPM. We prove that the minimax optimal rates for these two problems are multiplicatively equivalent, up to a \$\textbackslash log \textbackslash log (n)/\textbackslash log (n)\$ factor.},
  archivePrefix = {arXiv},
  eprint = {1911.00730},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/FHRRQJCR/Liang - 2019 - Estimating Certain Integral Probability Metric (IP.pdf},
  journal = {arXiv:1911.00730},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@inproceedings{liang2019FisherraoMetric,
  ids = {liang2019FisherRaoMetric},
  title = {Fisher-Rao Metric, Geometry, and Complexity of Neural Networks},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
  author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {888--896},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity \textemdash{} the Fisher-Rao norm \textemdash{} that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.},
  chapter = {Machine Learning},
  date-added = {2020-04-10 12:58:44 -0500},
  date-modified = {2020-07-22 20:59:43 -0500},
  file = {/Users/tengyuan/Zotero/storage/7NHSTYWQ/Liang et al. - 2019 - Fisher-Rao Metric, Geometry, and Complexity of Neu.pdf},
  pdf = {http://proceedings.mlr.press/v89/liang19a/liang19a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{liang2019InteractionMatters,
  ids = {liang2019InteractionMatter,liang2019InteractionMattersa},
  title = {Interaction Matters: {{A}} Note on Non-Asymptotic Local Convergence of Generative Adversarial Networks},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
  author = {Liang, Tengyuan and Stokes, James},
  editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {907--915},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for four modified dynamics proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO), Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate. As a by-product, we present a new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017] with improved rates.},
  chapter = {Machine Learning},
  date-added = {2020-07-22 20:53:17 -0500},
  date-modified = {2020-07-22 20:59:36 -0500},
  file = {/Users/tengyuan/Zotero/storage/7JN7WBZT/Liang and Stokes - 2019 - Interaction Matters A Note on Non-asymptotic Loca.pdf},
  pdf = {http://proceedings.mlr.press/v89/liang19b/liang19b.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{liang2019StatisticalInference,
  ids = {liang2019StatisticalInference},
  title = {Statistical Inference for the Population Landscape via Moment-Adjusted Stochastic Gradients},
  author = {Liang, Tengyuan and Su, Weijie J.},
  year = {2019},
  volume = {81},
  pages = {431--456},
  doi = {10.1111/rssb.12313},
  abstract = {Summary Modern statistical inference tasks often require iterative optimization methods to compute the solution. Convergence analysis from an optimization viewpoint informs us only how well the solution is approximated numerically but overlooks the sampling nature of the data. In contrast, recognizing the randomness in the data, statisticians are keen to provide uncertainty quantification, or confidence, for the solution obtained by using iterative optimization methods. The paper makes progress along this direction by introducing moment-adjusted stochastic gradient descent: a new stochastic optimization method for statistical inference. We establish non-asymptotic theory that characterizes the statistical distribution for certain iterative methods with optimization guarantees. On the statistical front, the theory allows for model misspecification, with very mild conditions on the data. For optimization, the theory is flexible for both convex and non-convex cases. Remarkably, the moment adjusting idea motivated from `error standardization' in statistics achieves a similar effect to acceleration in first-order optimization methods that are used to fit generalized linear models. We also demonstrate this acceleration effect in the non-convex setting through numerical experiments.},
  bdsk-url-2 = {https://doi.org/10.1111/rssb.12313},
  date-added = {2020-07-22 21:17:08 -0500},
  date-modified = {2020-07-22 21:17:08 -0500},
  eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12313},
  file = {/Users/tengyuan/Zotero/storage/WMQWR7DU/Liang and Su - 2019 - Statistical inference for the population landscape.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Acceleration,Diffusion process,Discretized Langevin algorithm,Model misspecification,Non-asymptotic inference,Population landscape,Stochastic gradient methods},
  number = {2}
}

@article{liang2020JustInterpolate,
  ids = {liang2020JustInterpolate},
  title = {Just Interpolate: {{Kernel}} ``{{Ridgeless}}'' Regression Can Generalize},
  author = {Liang, Tengyuan and Rakhlin, Alexander},
  year = {2020},
  month = jun,
  volume = {48},
  pages = {1329--1347},
  publisher = {{The Institute of Mathematical Statistics}},
  doi = {10.1214/19-AOS1849},
  date-added = {2020-07-22 21:12:30 -0500},
  date-modified = {2020-07-22 21:12:30 -0500},
  file = {/Users/tengyuan/Zotero/storage/RIZSR7QD/Liang and Rakhlin - 2020 - Just interpolate Kernel “Ridgeless” regression ca.pdf},
  fjournal = {Annals of Statistics},
  journal = {The Annals of Statistics},
  number = {3},
  sici = {0090-5364(2020)48:3\textexclamdown 1329:JIKRRC\textquestiondown 2.0.CO;2-V}
}

@article{liang2020MehlerFormula,
  ids = {liang2020MehlerFormulaBranchingc},
  title = {Mehler's {{Formula}}, {{Branching Process}}, and {{Compositional Kernels}} of {{Deep Neural Networks}}},
  author = {Liang, Tengyuan and {Tran-Bach}, Hai},
  year = {2020},
  month = apr,
  abstract = {In this paper, we utilize a connection between compositional kernels and branching processes via Mehler's formula to study deep neural networks. This new probabilistic insight provides us a novel perspective on the mathematical role of activation functions in compositional neural networks. We study the unscaled and rescaled limits of the compositional kernels and explore the different phases of the limiting behavior, as the compositional depth increases. We investigate the memorization capacity of the compositional kernels and neural networks by characterizing the interplay among compositional depth, sample size, dimensionality, and non-linearity of the activation. Explicit formulas on the eigenvalues of the compositional kernel are provided, which quantify the complexity of the corresponding reproducing kernel Hilbert space. On the algorithmic front, we propose a new random features algorithm, which compresses the compositional layers by devising a new activation function.},
  archivePrefix = {arXiv},
  eprint = {2004.04767},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/Q33DAQF9/Liang and Tran-Bach - 2020 - Mehler's Formula, Branching Process, and Compositi.pdf},
  journal = {arXiv:2004.04767},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@inproceedings{liang2020MultipleDescent,
  ids = {liang2020MultipleDescentMinimumNorma},
  title = {On the {{Multiple Descent}} of {{Minimum}}-{{Norm Interpolants}} and {{Restricted Lower Isometry}} of {{Kernels}}},
  booktitle = {Proceedings of the 33rd Conference on Learning {{Theory}}},
  author = {Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu},
  year = {2020},
  month = jul,
  pages = {1--29},
  publisher = {{PMLR}},
  abstract = {We study the risk of minimum-norm interpolants of data in Reproducing Kernel Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape for the various scalings of \$d = n\^\{\textbackslash alpha\}\$, \$\textbackslash alpha\textbackslash in(0,1)\$, for the input dimension \$d\$ and sample size \$n\$. Empirical evidence supports our finding that minimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in sample size; furthermore, locations of the peaks in our experiments match our theoretical predictions. Since gradient flow on appropriately initialized wide neural networks converges to a minimum-norm interpolant with respect to a certain kernel, our analysis also yields novel estimation and generalization guarantees for these over-parametrized models. At the heart of our analysis is a study of spectral properties of the random kernel matrix restricted to a filtration of eigen-spaces of the population covariance operator, and may be of independent interest.},
  archivePrefix = {arXiv},
  eprint = {1908.10292},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/ZR8DGVZ2/Liang et al. - 2020 - On the Multiple Descent of Minimum-Norm Interpolan.pdf},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  series = {Proceedings of Machine Learning Research}
}

@article{liang2020PreciseHighDimensional,
  ids = {liang2020PreciseHighDimensionalAsymptoticc},
  title = {A {{Precise High}}-{{Dimensional Asymptotic Theory}} for {{Boosting}} and {{Minimum}}-{{L1}}-{{Norm Interpolated Classifiers}}},
  author = {Liang, Tengyuan and Sur, Pragya},
  year = {2020},
  month = feb,
  abstract = {This paper establishes a precise high-dimensional asymptotic theory for boosting on separable data, taking statistical and computational perspectives. We consider the setting where the number of features (weak learners) \$p\$ scales with the sample size \$n\$, in an over-parametrized regime. Under a broad class of statistical models, we provide an exact analysis of the generalization error of boosting, when the algorithm interpolates the training data and maximizes the empirical \$\textbackslash ell\_1\$-margin. The relation between the boosting test error and the optimal Bayes error is pinned down explicitly. In turn, these precise characterizations resolve several open questions raised in \textbackslash cite\{breiman1999prediction, schapire1998boosting\} surrounding boosting. On the computational front, we provide a sharp analysis of the stopping time when boosting approximately maximizes the empirical \$\textbackslash ell\_1\$ margin. Furthermore, we discover that the larger the overparametrization ratio \$p/n\$, the smaller the proportion of active features (with zero initialization), and the faster the optimization reaches interpolation. At the heart of our theory lies an in-depth study of the maximum \$\textbackslash ell\_1\$-margin, which can be accurately described by a new system of non-linear equations; we analyze this margin and the properties of this system, using Gaussian comparison techniques and a novel uniform deviation argument. Variants of AdaBoost corresponding to general \$\textbackslash ell\_q\$ geometry, for \$q {$>$} 1\$, are also presented, together with an exact analysis of the high-dimensional generalization and optimization behavior of a class of these algorithms.},
  archivePrefix = {arXiv},
  eprint = {2002.01586},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/JDQ7X75V/Liang and Sur - 2020 - A Precise High-Dimensional Asymptotic Theory for B.pdf},
  journal = {arXiv:2002.01586},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@inproceedings{tzen2018LocalOptimality,
  title = {Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability},
  booktitle = {Proceedings of the 31st Conference on Learning Theory},
  author = {Tzen, Belinda and Liang, Tengyuan and Raginsky, Maxim},
  editor = {Bubeck, S{\'e}bastien and Perchet, Vianney and Rigollet, Philippe},
  year = {2018},
  month = jul,
  volume = {75},
  pages = {857--875},
  publisher = {{PMLR}},
  abstract = {We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \emph{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the {$\varepsilon$}-neighborhood of this particular optimum within a short \emph{recurrence time}; (2) it enters this {$\varepsilon$}-neighborhood by the recurrence time and stays there until a potentially exponentially long \emph{escape time}. We call this phenomenon \emph{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality.},
  date-added = {2020-07-22 20:53:24 -0500},
  date-modified = {2020-07-22 20:53:24 -0500},
  file = {/Users/tengyuan/Zotero/storage/P5UFPYZY/Tzen et al. - 2018 - Local Optimality and Generalization Guarantees for.pdf},
  pdf = {http://proceedings.mlr.press/v75/tzen18a/tzen18a.pdf},
  series = {Proceedings of Machine Learning Research}
}


